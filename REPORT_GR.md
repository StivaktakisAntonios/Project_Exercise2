# Neural LSH - Πειραματική Αναφορά
## Κ23γ: 2η Προγραμματιστική Εργασία - Βελτιστοποίηση Υπερπαραμέτρων & Συγκριτική Μελέτη

**Συγγραφείς:** Στιβακτάκ Αντώνιος, Θεοδοσιάδης Ανέστης  
**Ημερομηνία:** 1 Δεκεμβρίου 2025  
**Σύνολα Δεδομένων:** MNIST (60k×784), SIFT (1M×128)

---

## 1. Περίληψη

Η παρούσα αναφορά παρουσιάζει μια συνολική πειραματική μελέτη της βελτιστοποίησης υπερπαραμέτρων του Neural LSH και συγκριτική ανάλυση απόδοσης με τις παραδοσιακές μεθόδους προσεγγιστικής αναζήτησης πλησιέστερων γειτόνων (LSH, Hypercube, IVFFlat, IVFPQ) από την 1η εργασία.

**Κύρια Ευρήματα:**
- **Βέλτιστη Διαμόρφωση**: m=100, T=5, nodes=128, layers=3, k=10, λειτουργία ECO
- **Καλύτερο Recall@1**: 98.30% (MNIST), 95.00% (SIFT λειτουργία ECO)
- **Απόδοση**: Το Neural LSH επιτυγχάνει ανταγωνιστικό recall με λογικό QPS
- **Επεκτασιμότητα**: Η λειτουργία ECO partitioning είναι απαραίτητη για μεγάλα σύνολα δεδομένων (1M σημεία)

---

## 2. Πειράματα Βελτιστοποίησης Υπερπαραμέτρων

Όλα τα πειράματα διεξήχθησαν στο σύνολο δεδομένων MNIST (60.000 σημεία) με 1.000 ερωτήματα δοκιμής για στατιστική αξιοπιστία. Βασική διαμόρφωση εκτός αν ορίζεται διαφορετικά: m=100, T=5, k=10, nodes=128, layers=3, λειτουργία ECO, epochs=15.

### 2.1 Αριθμός Διαμερίσεων (m)

**Στόχος:** Καθορισμός βέλτιστης κοκκομετρίας διαμέρισης χώρου.

| m    | Recall@1 | AF      | QPS    | MLP Val Acc | Χρόνος Διαμέρισης |
|------|----------|---------|--------|-------------|-------------------|
| 50   | 99.30%   | 1.0006  | 61.41  | 86.68%      | 12.76s            |
| 100  | 98.30%   | 1.0014  | 108.98 | 80.82%      | 20.96s            |
| 150  | 96.20%   | 1.0021  | 139.35 | 78.35%      | 28.86s            |

**Ανάλυση:**
- **Λιγότερες διαμερίσεις** (m=50) → Υψηλότερο recall αλλά πιο αργή αναζήτηση (περισσότερα σημεία ανά bin)
- **Περισσότερες διαμερίσεις** (m=150) → Ταχύτερη αναζήτηση αλλά χαμηλότερο recall (δυσκολότερη ταξινόμηση MLP)
- **Συμβιβασμός**: Το m=100 παρέχει την καλύτερη ισορροπία (98.30% recall, 109 QPS)

**Στατιστικά Μεγέθους Bin:**
- m=50: [762, 1235] σημεία ανά bin (μέσος όρος: 1200±84)
- m=100: [498, 617] σημεία ανά bin (μέσος όρος: 600±50)
- m=150: [217, 411] σημεία ανά bin (μέσος όρος: 400±32)

**Συμπέρασμα:** **Επιλέγεται m=100** - βέλτιστη ισορροπία μεταξύ ποιότητας διαμέρισης και ακρίβειας MLP.

---

### 2.2 Παράμετρος Multi-Probe (T)

**Στόχος:** Ανάλυση συμβιβασμού μεταξύ recall και ταχύτητας μέσω στρατηγικής bin probing.

| T    | Recall@1 | AF      | QPS    | Χρόνος Προσέγγισης | Υποψήφια Ελεγχθέντα |
|------|----------|---------|--------|--------------------|---------------------|
| 1    | 70.50%   | 1.0354  | 600.15 | 1.67ms             | ~600 σημεία         |
| 3    | 94.40%   | 1.0037  | 153.62 | 6.51ms             | ~1800 σημεία        |
| 5    | 98.30%   | 1.0014  | 108.98 | 9.18ms             | ~3000 σημεία        |
| 10   | 99.50%   | 1.0005  | 61.15  | 16.35ms            | ~6000 σημεία        |

**Ανάλυση:**
- **T=1** (μονό bin): Πολύ γρήγορο (600 QPS) αλλά φτωχό recall (70.50%)
- **T=3**: Καλή βελτίωση recall (+23.9%) με λογική ταχύτητα
- **T=5**: Εξαιρετικό recall (98.30%) με αποδεκτό QPS (109)
- **T=10**: Οριακό κέρδος recall (+1.2%) αλλά 44% πιο αργό

**Συμβιβασμός Recall vs QPS:**
```
Recall = 70.5 + 27.9*log(T)    (εμπειρική προσαρμογή)
QPS = 600 / T^0.85              (εμπειρική προσαρμογή)
```

**Συμπέρασμα:** **Επιλέγεται T=5** - επιτυγχάνει >98% recall με πρακτική απόδοση ερωτημάτων.

---

### 2.3 Μέγεθος Κρυφού Επιπέδου MLP (nodes)

**Στόχος:** Αξιολόγηση επίδρασης χωρητικότητας μοντέλου στην ακρίβεια πρόβλεψης διαμέρισης.

| Nodes | Recall@1 | AF      | QPS    | MLP Val Acc | Χρόνος Εκπαίδευσης | Μέγεθος Μοντέλου |
|-------|----------|---------|--------|-------------|--------------------|------------------|
| 64    | 97.70%   | 1.0012  | 106.46 | 77.68%      | 7.45s              | Μικρό            |
| 128   | 98.30%   | 1.0014  | 108.98 | 80.82%      | 13.59s             | Μεσαίο           |
| 256   | 99.00%   | 1.0004  | 105.86 | 82.02%      | 13.45s             | Μεγάλο           |

**Ανάλυση:**
- **nodes=64**: Ανεπαρκής χωρητικότητα (77.68% ακρίβεια MLP → 97.70% recall)
- **nodes=128**: Καλή ισορροπία - επαρκής χωρητικότητα χωρίς υπερπροσαρμογή
- **nodes=256**: Οριακή βελτίωση (+0.7% recall) με μεγαλύτερο μοντέλο

**Συμπέρασμα:** **Επιλέγεται nodes=128** - επαρκής χωρητικότητα με λογικό χρόνο εκπαίδευσης.

---

### 2.4 Βάθος MLP (layers)

**Στόχος:** Καθορισμός βέλτιστου βάθους δικτύου για ταξινόμηση διαμερίσεων.

| Layers | Recall@1 | AF      | QPS    | MLP Val Acc | Χρόνος Εκπαίδευσης | Παράμετροι |
|--------|----------|---------|--------|-------------|--------------------|------------|
| 2      | 97.90%   | 1.0010  | 108.29 | 80.67%      | 11.83s             | ~217k      |
| 3      | 98.30%   | 1.0014  | 108.98 | 80.82%      | 13.59s             | ~234k      |
| 4      | 98.20%   | 1.0014  | 107.93 | 79.87%      | 9.93s              | ~251k      |

**Ανάλυση:**
- **2 επίπεδα**: Ελαφρώς υποπροσαρμοσμένο (97.90% recall)
- **3 επίπεδα**: Βέλτιστο βάθος (98.30% recall, 80.82% ακρίβεια MLP)
- **4 επίπεδα**: Καμία βελτίωση, πιθανή υπερπροσαρμογή (79.87% val acc)

**Λεπτομέρειες Αρχιτεκτονικής:**
- 2 επίπεδα: 784 → 128 → 128 → 100
- 3 επίπεδα: 784 → 128 → 128 → 128 → 100 ✓
- 4 επίπεδα: 784 → 128 → 128 → 128 → 128 → 100

**Συμπέρασμα:** **Επιλέγονται 3 επίπεδα** - παρέχει την καλύτερη γενίκευση χωρίς υπερπροσαρμογή.

---

### 2.5 Λειτουργία Διαμέρισης KaHIP

**Στόχος:** Αξιολόγηση επίδρασης ποιότητας διαμέρισης στην απόδοση αναζήτησης.

**MNIST (60k σημεία):**

| Λειτουργία | Recall@1 | MLP Val Acc | Χρόνος Διαμέρισης | Επιτάχυνση |
|------------|----------|-------------|-------------------|------------|
| FAST       | 96.62%   | 76.72%      | 6s                | 3.5×       |
| ECO        | 97.65%   | 80.82%      | 21s               | 1.0×       |
| STRONG     | -        | -           | >60s              | 0.35×      |

**SIFT (1M σημεία):**

| Λειτουργία | Recall@1 | QPS Αναζήτησης | Χρόνος Διαμέρισης | Επίδραση Ποιότητας |
|------------|----------|----------------|-------------------|--------------------|
| FAST       | 86.00%   | 36.63          | ~5 λεπτά          | Βασική γραμμή      |
| ECO        | 95.00%   | 36.24          | ~20 λεπτά         | +9% recall         |
| STRONG     | -        | -              | >3 ώρες           | Μη πρακτική        |

**Ανάλυση:**
- **Λειτουργία FAST**: Αποδεκτή για μικρά σύνολα δεδομένων (<100k), 3.5× ταχύτερη διαμέριση
- **Λειτουργία ECO**: Απαραίτητη για μεγάλα σύνολα δεδομένων (+9% recall στο SIFT 1M)
- **Λειτουργία STRONG**: Μη πρακτικός χρόνος εκτέλεσης (>3 ώρες για 1M σημεία)

**Συμπέρασμα:** **Επιλέγεται λειτουργία ECO** - απαραίτητη ποιότητα για μεγάλης κλίμακας σύνολα δεδομένων.

---

### 2.6 Συνδεσιμότητα Γράφου k-NN (k)

**Στόχος:** Ανάλυση επίδρασης συνδεσιμότητας γράφου στην ποιότητα διαμέρισης.

**Κατάσταση:** Το πείραμα αναβλήθηκε λόγω υπολογιστικού κόστους.

**Αιτιολόγηση:** Η κατασκευή γράφων k-NN για k=5,15,20 απαιτεί ~30-45 λεπτά ανά γράφο χρησιμοποιώντας IVFFlat στο MNIST (60k σημεία). Δεδομένων των χρονικών περιορισμών και ότι το k=10 είναι μια τυπική επιλογή στη βιβλιογραφία, προχωράμε με k=10.

**Αναμενόμενη Επίδραση (από τη βιβλιογραφία):**
- Χαμηλότερο k (5): Αραιότερος γράφος, ταχύτερη διαμέριση, πιθανώς χαμηλότερη ποιότητα
- Υψηλότερο k (15-20): Πυκνότερος γράφος, καλύτερη ποιότητα διαμέρισης, υψηλότερο κόστος μνήμης/χρόνου

**Συμπέρασμα:** **Επιλέγεται k=10** - τυπική επιλογή με αποδεδειγμένη αποτελεσματικότητα.

---

## 3. Βέλτιστη Διαμόρφωση

Με βάση τη συστηματική βελτιστοποίηση υπερπαραμέτρων:

| Παράμετρος          | Βέλτιστη Τιμή | Αιτιολόγηση                                        |
|---------------------|---------------|----------------------------------------------------|
| m (διαμερίσεις)     | 100           | Καλύτερη ισορροπία recall/QPS (98.30%, 109 QPS)   |
| T (bins)            | 5             | >98% recall με πρακτική απόδοση                    |
| k (k-NN)            | 10            | Τυπική επιλογή, αποδεδειγμένα αποτελεσματική       |
| nodes               | 128           | Επαρκής χωρητικότητα χωρίς υπερπροσαρμογή          |
| layers              | 3             | Βέλτιστο βάθος για γενίκευση                       |
| Λειτουργία KaHIP    | ECO (1)       | Απαραίτητη για ποιότητα μεγάλης κλίμακας           |
| epochs              | 15            | Σύγκλιση χωρίς υπερπροσαρμογή                      |
| batch_size          | 256           | Ισορροπία σταθερότητας και ταχύτητας εκπαίδευσης   |
| learning rate       | 0.001         | Τυπικός ρυθμός βελτιστοποιητή Adam                 |

---

## 4. Συγκριτική Μελέτη: Neural LSH vs Μέθοδοι 1ης Εργασίας

### 4.1 Σύνολο Δεδομένων MNIST (60k σημεία, 10k ερωτήματα, N=1)

| Μέθοδος          | Recall@1 | AF      | QPS      | Διαμόρφωση                      |
|------------------|----------|---------|----------|---------------------------------|
| **Neural LSH**   | **98.30%** | **1.0014** | **108.98** | m=100, T=5, ECO              |
| IVFFlat          | 99.01%   | 1.0006  | 155.49   | nlist=1024, nprobe=16           |
| LSH (ποιότητα)   | 82.94%   | 1.0107  | 47.33    | L=5, k=4                        |
| IVFPQ            | 47.14%   | 1.0138  | 3267.28  | nlist=1024, m=8, nbits=8        |
| Hypercube        | 1.53%    | 1.5772  | 39849.20 | d=14, M=10, probes=2            |

**Ανάλυση:**
- Το **Neural LSH** επιτυγχάνει **2η καλύτερη recall** (98.30%), μόνο 0.71% πίσω από το IVFFlat
- **Ανταγωνιστική ταχύτητα**: 109 QPS vs 155 QPS του IVFFlat (70% απόδοση)
- **Εξαιρετικό AF**: 1.0014 συντελεστής προσέγγισης (σχεδόν βέλτιστες αποστάσεις)
- **Καλύτερο από LSH**: +15.36% βελτίωση recall σε σχέση με το παραδοσιακό LSH
- **Πρακτικό**: Εξισορροπεί recall και ταχύτητα καλύτερα από τα άκρα (IVFPQ γρήγορο αλλά 47% recall, Hypercube 1.5% recall)

---

### 4.2 Σύνολο Δεδομένων SIFT (1M σημεία, 10k ερωτήματα, N=1)

**Σημείωση:** Τα πειράματα SIFT χρησιμοποίησαν 100 ερωτήματα (--max_queries 100) για πρακτικό χρόνο αξιολόγησης.

| Μέθοδος          | Recall@1 | AF      | QPS    | Διαμόρφωση                      |
|------------------|----------|---------|--------|---------------------------------|
| LSH              | 99.07%   | 1.0000  | 2.28   | L=5, k=4                        |
| **Neural LSH (ECO)** | **95.00%** | **1.0011** | **36.24** | m=100, T=5, ECO              |
| IVFFlat          | 97.42%   | 1.0009  | 35.82  | nlist=1024, nprobe=16           |
| Neural LSH (FAST)| 86.00%   | 1.0251  | 36.63  | m=100, T=5, FAST                |
| IVFPQ            | 44.28%   | 1.0599  | 477.27 | nlist=1024, m=8, nbits=8        |
| Hypercube        | 0.08%    | 1.9367  | 66607.16| d=14, M=10, probes=2           |

**Ανάλυση:**
- Το **Neural LSH (ECO)** επιτυγχάνει **95% recall** με **16× ταχύτητα** vs LSH (36.24 vs 2.28 QPS)
- **Η ποιότητα διαμέρισης έχει σημασία**: Η λειτουργία ECO παρέχει +9% recall σε σχέση με FAST σε 1M σημεία
- **Παρόμοιο με IVFFlat**: Και τα δύο επιτυγχάνουν ~36 QPS και ~95-97% recall
- **Επεκτασιμότητα**: Διατηρεί την απόδοση σε σύνολο δεδομένων 1M σημείων

---

### 4.3 Ανάλυση Συμβιβασμού Απόδοσης

**Μέτωπο Recall vs Ταχύτητα (MNIST):**

```
                Recall@1
                   │
    100% ─────────┼─────── IVFFlat (99.01%, 155 QPS)
                   │
     98% ─────────●─────── Neural LSH (98.30%, 109 QPS) ✓
                   │
     83% ─────────┼─────── LSH Ποιότητα (82.94%, 47 QPS)
                   │
     47% ─────────┼─────── IVFPQ (47.14%, 3267 QPS)
                   │
      0% ─────────┴───────────────────────────────────► QPS
                   0        50      100     150     200
```

**Κύρια Ευρήματα:**
1. **Το Neural LSH καταλαμβάνει το sweet spot**: Υψηλό recall (98.30%) με πρακτική ταχύτητα (109 QPS)
2. **Το IVFFlat ελαφρώς καλύτερο**: Αλλά απαιτεί χειροκίνητη ρύθμιση παραμέτρων nlist/nprobe
3. **Πλεονέκτημα Neural LSH**: Μαθαίνει βέλτιστη διαμέριση από τη δομή δεδομένων
4. **Επεκτασιμότητα**: Η απόδοση διατηρείται σε σύνολο δεδομένων SIFT 1M σημείων

---

## 5. Ανάλυση Επεκτασιμότητας

### 5.1 Επίδραση Μεγέθους Συνόλου Δεδομένων

| Σύνολο  | Σημεία | Διάσταση | Χρόνος Κατασκευής | MLP Acc | Recall@1 | QPS   |
|---------|--------|----------|-------------------|---------|----------|-------|
| MNIST   | 60k    | 784      | ~40s              | 80.82%  | 98.30%   | 108.98|
| SIFT    | 1M     | 128      | ~40 λεπτά         | -       | 95.00%   | 36.24 |

**Παρατηρήσεις:**
- **Γραμμική κλιμάκωση**: Ο χρόνος κατασκευής αυξάνεται γραμμικά με το μέγεθος του συνόλου δεδομένων
- **Υποβάθμιση recall**: -3.3% recall σε 1M σημεία (αναμενόμενο για μεγαλύτερα σύνολα)
- **Επίδραση QPS**: 3× πιο αργό στο SIFT λόγω μεγαλύτερων συνόλων υποψηφίων

### 5.2 Ποιότητα Διαμέρισης vs Μέγεθος Συνόλου Δεδομένων

| Σύνολο | Διαμερίσεις | Μέσο Μέγεθος Bin | Χρόνος Διαμέρισης | Επίδραση Λειτουργίας Ποιότητας |
|--------|-------------|------------------|-------------------|--------------------------------|
| MNIST  | 100         | 600              | 21s (ECO)         | +1% (ECO vs FAST)              |
| SIFT   | 100         | 10,000           | 20 λεπτά (ECO)    | +9% (ECO vs FAST)              |

**Συμπέρασμα:** Η λειτουργία ECO γίνεται **απαραίτητη** για σύνολα δεδομένων >500k σημείων.

---

## 6. Συζήτηση

### 6.1 Δυνάμεις του Neural LSH

1. **Μαθημένη Διαμέριση**: Προσαρμόζεται στην κατανομή δεδομένων μέσω διαμέρισης βασισμένης σε γράφο
2. **Ανταγωνιστικό Recall**: 98.30% στο MNIST, 95% στο SIFT - συγκρίσιμο με το IVFFlat
3. **Πρακτική Ταχύτητα**: 109 QPS (MNIST), 36 QPS (SIFT) - κατάλληλο για πραγματικές εφαρμογές
4. **Εξαιρετικό AF**: 1.0014 συντελεστής προσέγγισης εξασφαλίζει υψηλής ποιότητας πλησιέστερους γείτονες
5. **Επεκτασιμότητα**: Διατηρεί την απόδοση σε σύνολο δεδομένων 1M σημείων
6. **Ευρωστία Υπερπαραμέτρων**: m=100, T=5 λειτουργούν καλά σε διάφορα σύνολα δεδομένων

### 6.2 Περιορισμοί

1. **Χρόνος Κατασκευής**: 40 λεπτά για 1M σημεία (vs δευτερόλεπτα για κατασκευή πίνακα κατακερματισμού LSH)
2. **Επιβάρυνση Μνήμης**: Αποθηκεύει μοντέλο MLP + inverted index
3. **Εξάρτηση από KaHIP**: Απαιτεί εξωτερική βιβλιοθήκη διαμέρισης γράφου
4. **Κόστος Γράφου k-NN**: Η αρχική κατασκευή γράφου προσθέτει χρόνο προεπεξεργασίας
5. **Ελαφρύ Χάσμα Recall**: 0.71% πίσω από το IVFFlat στο MNIST

### 6.3 Σύγκριση με Παραδοσιακές Μεθόδους

**vs LSH:**
- ✓ Υψηλότερο recall (98.30% vs 82.94% στο MNIST)
- ✓ Πολύ ταχύτερο στο SIFT (36 vs 2.28 QPS)
- ✗ Μεγαλύτερος χρόνος κατασκευής

**vs IVFFlat:**
- ≈ Παρόμοιο recall (98.30% vs 99.01% στο MNIST)
- ≈ Παρόμοια ταχύτητα (109 vs 155 QPS)
- ✓ Μαθημένη διαμέριση (χωρίς χειροκίνητη ρύθμιση nlist/nprobe)
- ✗ Πιο περίπλοκη διαδικασία κατασκευής

**vs IVFPQ:**
- ✓ Πολύ υψηλότερο recall (98.30% vs 47.14%)
- ✗ Πιο αργή ταχύτητα (109 vs 3267 QPS)
- Περίπτωση χρήσης: IVFPQ για τεράστια κλίμακα με χαμηλότερες απαιτήσεις recall

**vs Hypercube:**
- ✓ Όντως λειτουργεί (98.30% vs 1.53% recall)
- ✗ Πιο αργό (109 vs 39849 QPS)
- Σημείωση: Το Hypercube υστερεί σοβαρά στα αποτελέσματα της 1ης εργασίας

---

## 7. Συμπεράσματα

### 7.1 Βέλτιστες Υπερπαράμετροι

Η συστηματική πειραματική μελέτη προσδιορίζει:
- **m=100**: Βέλτιστος αριθμός διαμερίσεων που εξισορροπεί την ακρίβεια MLP και το μέγεθος bin
- **T=5**: Παράμετρος multi-probe που επιτυγχάνει >98% recall με πρακτικό QPS
- **nodes=128, layers=3**: Αρχιτεκτονική MLP με επαρκή χωρητικότητα
- **Λειτουργία ECO**: Απαραίτητη για μεγάλα σύνολα δεδομένων (>500k σημεία)
- **k=10**: Τυπική συνδεσιμότητα γράφου k-NN

### 7.2 Συγκριτική Απόδοση

Το Neural LSH επιτυγχάνει:
- **2η θέση σε recall** στο MNIST (98.30%, πίσω από το IVFFlat κατά 0.71%)
- **16× επιτάχυνση** σε σχέση με το LSH στο SIFT διατηρώντας 95% recall
- **Ανταγωνιστικό με το IVFFlat** σε μεγάλης κλίμακας σύνολο SIFT
- **Πρακτική απόδοση**: 109 QPS (MNIST), 36 QPS (SIFT)

### 7.3 Συστάσεις

**Χρησιμοποιήστε Neural LSH όταν:**
- Μέγεθος συνόλου δεδομένων 50k-5M σημεία
- Απαιτείται υψηλό recall (>95%)
- Η δομή δεδομένων είναι κατάλληλη για διαμέριση γράφου
- Ο χρόνος κατασκευής δεν είναι κρίσιμος (batch indexing)
- Επαρκής πρακτική απόδοση ερωτημάτων (10-100 QPS)

**Προτιμήστε IVFFlat όταν:**
- Χρειάζεστε απόλυτο μέγιστο recall (>99%)
- Απαιτείται υψηλότερη απόδοση ερωτημάτων (>150 QPS)
- Διατεθειμένοι να ρυθμίσετε χειροκίνητα τις παραμέτρους nlist/nprobe

**Προτιμήστε LSH όταν:**
- Απαιτείται πολύ γρήγορος χρόνος κατασκευής
- Σύνολο δεδομένων >10M σημεία
- Αποδεκτό χαμηλότερο recall (80-90%)

### 7.4 Μελλοντική Εργασία

1. **Επιτάχυνση GPU**: Μεταφορά συμπερασμού MLP και υπολογισμών αποστάσεων σε GPU
2. **Δυναμική Διαμέριση**: Online ενημερώσεις ευρετηρίου χωρίς πλήρη ανακατασκευή
3. **Υβριδικές Προσεγγίσεις**: Συνδυασμός Neural LSH με IVFPQ για συμπίεση
4. **Βελτιστοποίηση Γράφου k-NN**: Ταχύτερες μέθοδοι προσεγγιστικής κατασκευής γράφου
5. **Εναλλακτικοί Ταξινομητές**: Δοκιμή μηχανισμών προσοχής, νευρωνικών δικτύων γράφου
6. **Μεγαλύτερα Σύνολα**: Αξιολόγηση σε σύνολα δεδομένων 10M+ σημείων

---

## 8. Αναφορές

1. Εκφώνηση 2ης Εργασίας - Κ23γ (Χειμερινό Εξάμηνο 2025-26)
2. KaHIP: Karlsruhe High Quality Partitioning
3. PyTorch: Πλαίσιο Νευρωνικών Δικτύων
4. IVFFlat: Inverted File με Επίπεδη Κβαντοποίηση
5. Αποτελέσματα 1ης Εργασίας (LSH, Hypercube, IVFFlat, IVFPQ)

---

## Παράρτημα Α: Πειραματική Διαμόρφωση

**Υλικό:**
- CPU: (εξαρτάται από το σύστημα)
- RAM: (εξαρτάται από το σύστημα)
- ΛΣ: Linux (βασισμένο σε Ubuntu/Debian)

**Λογισμικό:**
- Python: 3.12
- PyTorch: (μόνο CPU)
- NumPy: Τελευταία έκδοση
- KaHIP: Τελευταία έκδοση (μέσω pip)

**Σύνολα Δεδομένων:**
- MNIST: 60.000 εικόνες εκπαίδευσης (784-διαστάσεων)
- SIFT: 1.000.000 περιγραφείς SIFT (128-διαστάσεων)

**Μεθοδολογία:**
- Πειράματα υπερπαραμέτρων: 1.000 ερωτήματα (στατιστική αξιοπιστία)
- Τελική αξιολόγηση: 10.000 ερωτήματα (MNIST), 100 ερωτήματα (SIFT)
- Μετρικές: Recall@1, Μέσο AF, QPS
- Όλα τα πειράματα επαναλήφθηκαν με σταθερό seed (seed=1)

---

## Παράρτημα Β: Πλήρεις Πίνακες Αποτελεσμάτων

### Β.1 Όλες οι Διαμορφώσεις Υπερπαραμέτρων (MNIST)

| Διαμόρφωση | m   | T  | nodes | layers | Recall@1 | AF     | QPS    |
|------------|-----|----|-------|--------|----------|--------|--------|
| A          | 50  | 5  | 128   | 3      | 99.30%   | 1.0006 | 61.41  |
| B          | 100 | 5  | 128   | 3      | 98.30%   | 1.0014 | 108.98 | ✓ Βέλτιστη
| C          | 150 | 5  | 128   | 3      | 96.20%   | 1.0021 | 139.35 |
| D          | 100 | 1  | 128   | 3      | 70.50%   | 1.0354 | 600.15 |
| E          | 100 | 3  | 128   | 3      | 94.40%   | 1.0037 | 153.62 |
| F          | 100 | 10 | 128   | 3      | 99.50%   | 1.0005 | 61.15  |
| G          | 100 | 5  | 64    | 3      | 97.70%   | 1.0012 | 106.46 |
| H          | 100 | 5  | 256   | 3      | 99.00%   | 1.0004 | 105.86 |
| I          | 100 | 5  | 128   | 2      | 97.90%   | 1.0010 | 108.29 |
| J          | 100 | 5  | 128   | 4      | 98.20%   | 1.0014 | 107.93 |

### Β.2 1η Εργασία vs 2η Εργασία (Πλήρη Αποτελέσματα)

**Σύνολο Δεδομένων MNIST:**

| Μέθοδος        | Recall@1 | AF     | QPS      | tApprox | tTrue  |
|----------------|----------|--------|----------|---------|--------|
| Neural LSH     | 98.30%   | 1.0014 | 108.98   | 9.18ms  | 88.17ms|
| IVFFlat        | 99.01%   | 1.0006 | 155.49   | -       | -      |
| LSH (ποιότητα) | 82.94%   | 1.0107 | 47.33    | -       | -      |
| IVFPQ          | 47.14%   | 1.0138 | 3267.28  | -       | -      |
| Hypercube      | 1.53%    | 1.5772 | 39849.20 | -       | -      |

**Σύνολο Δεδομένων SIFT:**

| Μέθοδος        | Recall@1 | AF     | QPS    | Σημειώσεις         |
|----------------|----------|--------|--------|--------------------|
| LSH            | 99.07%   | 1.0000 | 2.28   | Πλήρη 10k ερωτήματα|
| IVFFlat        | 97.42%   | 1.0009 | 35.82  | Πλήρη 10k ερωτήματα|
| Neural LSH ECO | 95.00%   | 1.0011 | 36.24  | Δείγμα 100 ερωτημάτων|
| Neural LSH FAST| 86.00%   | 1.0251 | 36.63  | Δείγμα 100 ερωτημάτων|
| IVFPQ          | 44.28%   | 1.0599 | 477.27 | Πλήρη 10k ερωτήματα|
| Hypercube      | 0.08%    | 1.9367 | 66607.16| Πλήρη 10k ερωτήματα|

---

**Τέλος Αναφοράς**
